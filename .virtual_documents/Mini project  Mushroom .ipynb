








# Library import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import chi2_contingency

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder



# utils contains some custom functions
from utils import full_frequency_analysis

# show everything in pd
pd.set_option("display.max_colwidth", None)
pd.set_option("display.max_columns", None)
# restore default settings
# pd.reset_option('all')


columns = [
    "poisonous",
    "cap-shape",
    "cap-surface",
    "cap-color",
    "bruises",
    "odor",
    "gill-attachment",
    "gill-spacing",
    "gill-size",
    "gill-color",
    "stalk-shape",
    "stalk-root",
    "stalk-surface-above-ring",
    "stalk-surface-below-ring",
    "stalk-color-above-ring",
    "stalk-color-below-ring",
    "veil-type",
    "veil-color",
    "ring-number",
    "ring-type",
    "spore-print-color",
    "population",
    "habitat",
]

data = pd.read_csv(
    "mushroom/agaricus-lepiota.data",
    header=None,
    names=columns,
)



data.replace("?", pd.NA, inplace=True)
print("------------------data info----------------")
print(data.info())
print("-------------------Missing Values--------------------------")
print(data.isnull().sum())
print("-------------------Data head--------------------------")
data.head()








data["stalk-root"] = data["stalk-root"].fillna("missing")
print(data["stalk-root"])





data.duplicated().sum()











print(data.describe())








summary_dict = full_frequency_analysis(data)
summary_dict








fig, axes = plt.subplots(nrows=8, ncols=3, figsize=(18, 30))
axes = axes.flatten()

for i, col in enumerate(data.columns):
    sns.countplot(x=col, data=data, hue=col, palette="viridis", ax=axes[i])
    axes[i].set_title(f"Distribution of {col}")
    axes[i].tick_params(axis="x", rotation=45)

for j in range(i + 1, len(axes)):
    axes[j].axis("off")

plt.subplots_adjust(hspace=0.4)
plt.tight_layout()
# plt.savefig("plots/distributions",dpi=150)
plt.show()











features = data.columns.drop("poisonous")
# Plot settings
plt.figure(figsize=(15, 30))  # Large canvas for multiple plots
axes = axes.flatten()
for i, feature in enumerate(features, 1):
    plt.subplot(8, 3, i)
    sns.countplot(
        data=data, x=feature, hue="poisonous", palette={"e": "lightgreen", "p": "red"}
    )
    plt.title(f"{feature} vs Poisonous")
    plt.xticks(rotation=45)
    plt.tight_layout()

plt.subplots_adjust(hspace=0.4)
# plt.savefig("plots/all_stacked_bar_plots.png", dpi=50)  # Save as image
plt.show()









results = []
# Loop through each feature
for feature in data.columns.drop("poisonous"):
    # Create contingency table
    contingency_table = pd.crosstab(data[feature], data["poisonous"])

    # Run Chi-Square test
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    results.append(
        {"Feature": feature, "Chi2": chi2, "p-value": p, "Significant": p < 0.05}
    )

chi2results = pd.DataFrame(results).sort_values("Chi2", ascending=False)
chi2results








#dropped odor to confuse the model a little bit 
to_drop = ["veil-type", "gill-attachment", "stalk-shape", "odor"]
original_data = data.copy()
data = data.drop(columns=to_drop)
data.columns


X = data.drop(columns=["poisonous"])  # Features
y = data["poisonous"]  # Target






X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # Maintains class distribution
)





from sklearn.preprocessing import LabelEncoder

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # Maintains class distribution
)
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)
le.classes_





X_train_encoded = pd.get_dummies(X_train, prefix_sep="_")
X_test_encoded = pd.get_dummies(X_test, prefix_sep="_")


# Ensure test set has same columns as train set
missing_cols = set(X_train_encoded.columns) - set(X_test_encoded.columns)
for col in missing_cols:
    X_test_encoded[col] = 0  # Add missing columns with 0 values
X_test_encoded = X_test_encoded[X_train_encoded.columns]  # Reorder columns


print(f"Training shape: {X_train_encoded.shape}, Test shape: {X_test_encoded.shape}")





from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# PCA needs to be performed on scaled data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

pca = PCA(n_components=0.95, random_state=42)
X_train_pca = pca.fit_transform(X_train_scaled)
print(f"Original features: {X_train_encoded.shape[1]}")
print(f"Reduced components: {pca.n_components_}")





pca_components = pd.DataFrame(
    pca.components_,
    columns=X_train_encoded.columns,
    index=[f"PC_{i+1}" for i in range(pca.n_components_)],
)
pca_components.head()





i = 0 #index of the PC that I am going to use
plt.figure(figsize=(12, 8))
top_features = pca_components.iloc[i].abs().sort_values(ascending=False).head(5)
sns.barplot(x=top_features.index, y=top_features.values)
plt.title(
    f"Top Features for PC_{i+1} (Variance: {pca.explained_variance_ratio_[i]:.1%})"
)
plt.xticks(rotation=45)
plt.show()





feature_importance = pd.DataFrame(
    {
        "feature": X_train_encoded.columns,
        "importance": np.sum(np.abs(pca.components_), axis=0),
    }
).sort_values("importance", ascending=False)
feature_importance["Cumulative_Importance in %"] = (
    feature_importance["importance"].cumsum() / feature_importance["importance"].sum() * 100
)
feature_importance.reset_index(drop=True).T








top_features = feature_importance[
    feature_importance["Cumulative_Importance in %"] <= 80
]
features_tokeep = top_features["feature"].tolist()
print("number of features to keep: " + str(len(features_tokeep)))
features_tokeep





X_train_reduced = X_train_encoded[features_tokeep]
X_test_reduced = X_test_encoded[features_tokeep]


print(
    f"New Training shape: {X_train_reduced.shape}, New Test shape: {X_test_reduced.shape}"
)


import pandas as pd
import numpy as np

# Assuming these are numpy arrays
# If they are not, you can convert them to numpy arrays first

# Stack the datasets
X = np.vstack([X_train_reduced, X_test_reduced])  # Stack features vertically
y = np.concatenate([y_train_encoded, y_test_encoded])  # Stack labels

# Combine into one DataFrame
combined = pd.DataFrame(X, columns=features_tokeep)
combined['target'] = y  # Add the target column


# Save to CSV
combined.to_csv('mushroom/full_dataset.csv', index=False)













# Set up KNN classifier
knn = KNeighborsClassifier()

# Define parameters to test
param_grid = {
    "n_neighbors": [3, 5, 7, 9],
    "weights": ["uniform", "distance"],
    "metric": [ "hamming", "chebyshev",'manhattan'],
}

# Run grid search with 5-fold cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring="accuracy", n_jobs=-1)

# Train the model
grid_search.fit(X_train_reduced, y_train_encoded)

# Show best results
print("Best Parameters:", grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

# Test the best model
best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(X_test_reduced)
print("Test Accuracy:", accuracy_score(y_test_encoded, y_pred))
print(
    "\nClassification Report:\n",
    classification_report(y_test_encoded, y_pred, target_names=le.classes_),
)



# Create confusion matrix
cm = confusion_matrix(y_test_encoded, y_pred)

# Plot heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=le.classes_,
    yticklabels=le.classes_,
)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()


#ROC and AUC Calculation
# Get predicted probabilities for the positive class
y_prob = best_knn.predict_proba(X_test_reduced)[:, 1]  # Probability for the positive class

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test_encoded, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.6f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positivwe Rate')
plt.title('(ROC) Curve + AUC')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()








dt = DecisionTreeClassifier(random_state=42)
param_grid = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

# Run grid search with 5-fold cross-validation
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train_reduced, y_train_encoded)

# Best pre-pruning parameters
print("Best Pre-pruning Parameters:", grid_search.best_params_)
print("Best CV Score (Pre-pruning):", grid_search.best_score_)

# Get the best pre-pruned tree
best_prepruned_dt = grid_search.best_estimator_








# Post-pruning: Compute the cost complexity pruning path
path = best_prepruned_dt.cost_complexity_pruning_path(X_train_reduced, y_train_encoded)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

clfs = []
train_scores = []
test_scores = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(
        random_state=42,
        ccp_alpha=ccp_alpha,
        **{k: v for k, v in grid_search.best_params_.items() if k != 'criterion'},
        criterion=grid_search.best_params_['criterion']
    )
    clf.fit(X_train_reduced, y_train_encoded)
    clfs.append(clf)
    train_scores.append(clf.score(X_train_reduced, y_train_encoded))
    test_scores.append(clf.score(X_test_reduced, y_test_encoded))


# Plot the pruning curve (accuracy vs ccp_alpha)
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_scores, marker='o', label='Train Accuracy', drawstyle='steps-post')
plt.plot(ccp_alphas, test_scores, marker='o', label='Test Accuracy', drawstyle='steps-post')
plt.xlabel('ccp_alpha')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. ccp_alpha for Post-Pruning')
plt.legend()
plt.grid(True)
plt.show()


best_ccp_alpha = ccp_alphas[np.argmax(test_scores)]
print("Best ccp_alpha:", best_ccp_alpha)
print("Best Test Accuracy from Pruning Curve:", max(test_scores))





final_dt = DecisionTreeClassifier(
    random_state=42,
    ccp_alpha=best_ccp_alpha,
    **{k: v for k, v in grid_search.best_params_.items() if k != 'criterion'},
    criterion=grid_search.best_params_['criterion']
)
final_dt.fit(X_train_reduced, y_train_encoded)
# Evaluate the final model
y_pred = final_dt.predict(X_test_reduced)
print("Final Model Test Accuracy:", accuracy_score(y_test_encoded, y_pred))

print("\nClassification Report:\n", classification_report(y_test_encoded, y_pred, target_names=le.classes_))
y_prob_dt = final_dt.predict_proba(X_test_reduced)[:, 1]  # Probability for the positive class

fpr_dt, tpr_dt, _ = roc_curve(y_test_encoded, y_prob_dt)
roc_auc_dt = auc(fpr_dt, tpr_dt)

plt.figure(figsize=(8, 6))
plt.plot(fpr_dt, tpr_dt, color='orange', lw=2, label=f'Decision Tree (AUC = {roc_auc_dt:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Decision Tree')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print("AUC Score:", roc_auc_dt)


cm = confusion_matrix(y_test_encoded, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=le.classes_,
    yticklabels=le.classes_
)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()


# Visualize the decision tree
plt.figure(figsize=(20, 10))
plot_tree(
    final_dt,
    feature_names=X_train_reduced.columns if hasattr(X_train_reduced, 'columns') else [f'Feature {i}' for i in range(X_train_reduced.shape[1])],
    class_names=le.classes_,
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title('Decision Tree Visualization')
plt.show()





viz_dt = DecisionTreeClassifier(
    random_state=42,
    max_depth=3,  # Limit depth for better readability
    ccp_alpha=best_ccp_alpha,
    **{k: v for k, v in grid_search.best_params_.items() if k != 'criterion' and k != 'max_depth'},
    criterion=grid_search.best_params_['criterion']
)
viz_dt.fit(X_train_reduced, y_train_encoded)
plt.figure(figsize=(15, 10), dpi=300)  # High resolution for clarity
plot_tree(
    viz_dt,
    feature_names=X_train_reduced.columns if hasattr(X_train_reduced, 'columns') else [f'Feature {i}' for i in range(X_train_reduced.shape[1])],
    class_names=le.classes_,
    filled=True,
    rounded=True,
    fontsize=12,  # Larger font for readability
    proportion=True,  
    precision=2, 
    impurity=True, 
    node_ids=True,  
    max_depth=3 
)
plt.title('Simplified Decision Tree Visualization (Limited Depth)', fontsize=16, pad=20)
plt.tight_layout()

plt.savefig('plots/decision_tree_visualization.png', dpi=300, bbox_inches='tight')
plt.show()











#Importing necessary libraries
import importlib

from utils import GetDummiesTransformer , FeatureSelector #defined in utils.py
from sklearn.pipeline import Pipeline






features_afterdrop = list(set(features)-(set(['odor', 'gill-attachment', 'stalk-shape', 'veil-type'])))

print(features_afterdrop)
preprocessor = Pipeline([
    ('get_dummies', GetDummiesTransformer(columns=features_afterdrop))
])
knn_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('feature_selector', FeatureSelector(features_tokeep)),
    ('classifier', KNeighborsClassifier(metric='hamming', n_neighbors=9, weights='uniform'))
])

dt_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('feature_selector', FeatureSelector(features_tokeep)),
    ('classifier', DecisionTreeClassifier(
        criterion='gini', max_depth=None, min_samples_leaf=1, min_samples_split=10,
        ccp_alpha=3.517024597190772e-05, random_state=42
    ))
])


X = data.drop(columns=["poisonous"])  # Features
y = data["poisonous"]  # Target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y  # Maintains class distribution
)
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

print("X_train shape:", X_train.shape)
pd.DataFrame(X_test).head()





from sklearn import set_config

def evaluate_model(pipeline, X_train, X_test, y_train, y_test, model_name):
    # Fit the pipeline
    pipeline.fit(X_train, y_train_encoded)
    # Predict
    
   
    y_prob = pipeline.predict_proba(X_test)[:, 1]
    
    # Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} Test Accuracy: {accuracy:.4f}")
    
    # Classification Report
    print(f"\n{model_name} Classification Report:\n")
    print(classification_report(y_test, y_pred, target_names=le.classes_))
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=le.classes_, yticklabels=le.classes_
    )
    plt.title(f'{model_name} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()


set_config(enable_metadata_routing=True)
evaluate_model(knn_pipeline, X_train, X_test, y_train_encoded, y_test_encoded, "KNN")
evaluate_model(dt_pipeline, X_train, X_test, y_train_encoded, y_test_encoded, "Decision Tree")
